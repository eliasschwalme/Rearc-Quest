{
  "jobConfig": {
    "name": "s3_script",
    "description": "",
    "role": "arn:aws:iam::113524984329:role/s3_glue_quest",
    "command": "pythonshell",
    "version": "3.0",
    "workerType": null,
    "numberOfWorkers": null,
    "maxCapacity": 0.0625,
    "maxRetries": 3,
    "timeout": 2880,
    "maxConcurrentRuns": 1,
    "security": "none",
    "scriptName": "s3_script.py",
    "scriptLocation": "s3://aws-glue-assets-113524984329-us-east-1/scripts/",
    "language": "python-3.9",
    "jobParameters": [],
    "tags": [],
    "jobMode": "DEVELOPER_MODE",
    "developerMode": true,
    "connectionsList": [],
    "temporaryDirectory": "s3://aws-glue-assets-113524984329-us-east-1/temporary/",
    "glueHiveMetastore": true,
    "etlAutoTuning": false,
    "pythonShellPrebuiltLibraryOption": "analytics",
    "flexExecution": false,
    "minFlexWorkers": null,
    "sourceControlDetails": {
      "Provider": "GITHUB",
      "Repository": "Rearc-Quest",
      "Branch": "aws_glue"
    }
  },
  "hasBeenSaved": false,
  "script": "
  import re
  import csv
  import boto3
  import requests
  from bs4 import BeautifulSoup
  import functools
  import json
  
  
  # ### Class constructor
  
  class manage_s3():
      def __init__(self,bucket_name,url,key=None):
          self.bucket_name = bucket_name
          self.url = url
  
          # AWS credentials as not needed as this script will run on AWS.
          # For running on local machines please uncomment the following lines.
          
          if key:
              with open(key, \"r\") as f:
                  self.AK,self.SK = [x.split()[0] for x in f.readlines()[-1].split(\',\')]
              self.s3 = boto3.resource(\'s3\',                      
                  aws_access_key_id=self.AK, aws_secret_access_key=self.SK
              )
          else:
              self.s3 = boto3.resource(\'s3\')
  
  # ### Get the file names
  
      def get_name(self):
          soup = BeautifulSoup(requests.get(self.url).text, \"lxml\")
          print(\"Reading file names complete.\")
          return [page.string for page in soup.findAll(\'a\', href=re.compile(\'\'))[1:]]
  
  
  # ### Read the files from S3
  
      def read_s3(self):
          ret_dict = {}
          # Create bucket if not exist, else get the bucket.
          bucket = self.s3.create_bucket(Bucket=self.bucket_name)
          for i,obj in enumerate(bucket.objects.all()):
              ret_dict[obj.key] = obj.get()['Body'].read()
          print(\"Reading s3 complete.\")
          return ret_dict
  
  
  # ### Sync the files
  
      def sync_files(self):
          files = self.get_name()
          s3_files = self.read_s3()
          file_name = s3_files.keys()
          
          print(\"Uploading/Updating files to s3\")
          
          for i, f in enumerate(files):
              with requests.get(self.url+f, stream=True) as r:
                  if f not in file_name:
                      self.s3.Object(self.bucket_name, f).put(Body=r.content)
                      print(f\"{i+1}) {f} uploaded\")
                  else:
                      if r.content != s3_files[f]:
                          self.s3.Object(self.bucket_name, f).put(Body=r.content)
                          print(f\"{i+1}) {f} updated\")
                      else:
                          print(f\"{i+1}) {f} skipped\")
          
          print(\"Deleting files from s3\")
          
          del_f = [f for f in file_name if f not in files]
          for i, f in enumerate(del_f):
              self.s3.Object(self.bucket_name, f).delete()
              print(f\"{i+1}) {f} deleted\")
  
  # ### Execution
  
  bucket_name = \"s1quest\"
  res_url = \"https://download.bls.gov/pub/time.series/pr/\"
  
  s = manage_s3(bucket_name, res_url)
  s.sync_files()"
}
"
}